<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Q-Learning | Aadhar Chauhan à¤†à¤§à¤¾à¤° à¤šà¥Œà¤¹à¤¾à¤¨</title> <meta name="author" content="Aadhar Chauhan à¤†à¤§à¤¾à¤° à¤šà¥Œà¤¹à¤¾à¤¨"/> <meta name="description" content=""/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸš˜</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://aadharc.github.io/projects/Project_QL/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">AadharÂ </span>ChauhanÂ à¤†à¤§à¤¾à¤° à¤šà¥Œà¤¹à¤¾à¤¨</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/Repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Q-Learning</h1> <p class="post-description"></p> </header> <article> <p><strong>Introduction:</strong> In the field of artificial intelligence, reinforcement learning is a powerful technique that enables an agent to learn optimal actions through trial and error. In this blog post, weâ€™ll explore a simple yet effective example of reinforcement learning using Q-tables. Weâ€™ll train an agent to navigate a grid world, avoiding enemies and seeking rewards.</p> <p><strong>Setting up the Environment:</strong> Our grid world consists of a square grid with a specified size. The agent, represented by a blue blob, moves within this grid. We also have food items (green blobs) that provide rewards when reached and enemies (red blobs) that impose penalties when encountered.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0" style="text-align:center"> <figure> <picture> <img src="/assets/img/Grid_world.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Environment" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Fig 1: Our Grid World Environment. Green : Food, Red : Enemy and Blue : Agent. </div> <p><strong>Defining the Q-Table:</strong> To facilitate the learning process, we utilize a Q-table, which is a lookup table that maps state-action pairs to their corresponding Q-values. The Q-value represents the expected future reward for taking a specific action in a given state. Initially, the Q-table is randomly initialized.</p> <p><strong>Training the Agent:</strong> We run multiple episodes of interaction between the agent and the environment. In each episode, the agent selects actions based on an exploration-exploitation trade-off controlled by the epsilon parameter. If a random value is greater than epsilon, the agent exploits its current knowledge and chooses the action with the highest Q-value. Otherwise, it explores by selecting a random action.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0" style="text-align:center"> <figure> <picture> <img src="/assets/img/Video_12000_IL.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Agent in action" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Fig 2: Our agent in action. </div> <p>During the episode, the agent moves in the grid world, and its state changes based on the relative positions of the food and enemies. The agent updates its Q-values using the Q-learning algorithm, which incorporates the observed rewards and future Q-values. The learning rate and discount factor influence the weight given to immediate rewards and future rewards, respectively.</p> <p><strong>Visualizing the Training:</strong> To provide visual feedback, we use OpenCV and matplotlib. We create an image for each step of the episode, highlighting the agent, food, and enemies in different colors. These images are combined to create a video that showcases the agentâ€™s behavior and learning progress. We also plot a moving average of the episode rewards to track the agentâ€™s performance over time.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0" style="text-align:center"> <figure> <picture> <img src="/assets/img/MA_plot.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Moving Average" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Fig 3: The results after training. We have calculated moving average of reward at every 3000 episode and plotted it against No. of Episodes. </div> <p><strong>Conclusion:</strong> In this blog post, weâ€™ve explored the concept of reinforcement learning using Q-tables and applied it to train an agent in a grid world environment. By updating Q-values based on rewards and future expectations, the agent learns to make optimal decisions. Weâ€™ve visualized the training process using videos and plotted the episode rewards to analyze the agentâ€™s performance.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> Â© Copyright 2024 Aadhar Chauhan à¤†à¤§à¤¾à¤° à¤šà¥Œà¤¹à¤¾à¤¨. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Last updated: August 29, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>